# -*- coding: cp1252 -*-
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim import corpora, models, similarities

documents = ["Answers to clinical and public health research questions increasingly require aggregated data from multiple sites. Data from electronic health records and other clinical sources are useful for such studies, but require stringent quality assessment. Data quality assessment is particularly important in multisite studies to distinguish true variations in care from data quality problems.We propose a “fit-for-use” conceptual model for data quality assessment and a process model for planning and conducting single-site and multisite data quality assessments.These approaches are illustrated using examples from prior multisite studies.Critical components of multisite data quality assessment include: thoughtful prioritization of variables and data quality dimensions for assessment; development and use of standardized approaches to data quality assessment that can improve data utility over time;iterative cycles of assessment within and between sites; targetingassessment towards data domains known to be vulnerable to quality problems; and detailed documentation of the rationale and outcomes of data quality assessments toinform data users. The assessment process requires constant communication between site-level data providers, data coordinating centers, and principal investigators conceptually based and systematically executed approach to data quality assessment is essential to achieve the potential of the electronic revolution in health care.High quality data allow “learning health care organizations” to analyze and act on their own information, to compare their outcomes to peers, and to address criticalscientific questions from the population perspective",
"High quality routine health system data is essential for tracking progress towards attainment of the Millennium Development Goals 4 & 5. This study aimed to determine the completeness and accuracy of transfer of routine maternal health service data at health facility, districtregional levels of the Greater Accra Region of Ghana.cross sectional study was conducted using secondary data comprised of routine health information data collected atfacility level for the first quarter of 2012. Twelve health facilities were selected using a multistage sampling method. Data relating to antenatal care and delivery were assessed for completeness and accuracy of data transfer. Primary source data from health facility level (registers and record notebooks where health information data are initially entered) , used as the reference data, were counted, collated, and compared with aggregate data on aggregate forms compiled from these sources by health facilitystaff. The primary source data was also compared with data in the district health information management system (DHIMS–II), a web-based data collation and reporting system Percentage completeness and percentage error in data transfer were estimated.Data for all 5,537 antenatal registrants and 3, 466 deliveries recorded into the primary source first quarter of 2012 were assessed. Completeness was best for age data, followed by data on parity and hemoglobin at registration. Mean completeness of the facility level aggregate data for the data sampled, was 94.3% (95% CI = 90.6% – 98.0%) and 100.0% respectively for the aggregate form and DHIMS-II database. Mean error in data transfer was Percentage error comparing aggregate form data and DHIMS-II data respectively to the primary source data ranged from 0.0% to 4.9% respectively, while percentage error comparing the DHIMS-II data to aggregate form data, was generally very low or 0.0%. Routine maternal health services data in the Greater Accra region, available at the district level through the DHIMS-II system is complete when compared to facility level primary source data and reliable for use",
"Food environment characterization in health studies often requires" +
             "data on the location of food stores and restaurants. While commercial business lists are commonly used as data sources for such studies, current literature provides little guidance" +
             "on how to use validation study results to make decisions on which commercial business list to use and how to maximize the accuracy of those lists. Using data from a retrospective cohort" +
             "study [Weight And Veterans’ Environments Study (WAVES)], we (a) explain how validity and bias information from existing validation studies (count accuracy, classification accuracy," +
             "locational accuracy, as well as potential bias by neighborhood racial/ethnic composition, economic characteristics, and urbanicity) were used to determine which commercial business" +
             "listing to purchase for retail food outlet data and (b) describe the methods used to maximize the quality of the dataand results of this approach. We developed data improvement methods" +
             "based on existing validation studies. These methods included purchasing records from commercial business lists (InfoUSA and Dun and Bradstreet) based on store/restaurant names as well" +
             "as standard industrial classification (SIC) codes, reclassifying records by store type, improving geographic accuracy of records, and deduplicating records. We examined the impact" +
             "of these procedures on food outlet counts in US census tracts. After cleaning and deduplicating, our strategy resulted in a 17.5% reduction in the count of food stores that were valid" +
             "from those purchased from InfoUSA and 5.6% reduction in valid counts of restaurants purchased from Dun and Bradstreet. Locational accuracy was improved for 7.5% of records by applying" +
             "street addresses of subsequent years to records with post-office (PO) box addresses. In total, up to 83 of US census tracts annually experienced a change (either positive or negative)" +
             "in the count of retail food outlets between the initial purchase and the final datasets Our study provides a step-by-step approach to purchase and process business list data obtained" +
             "from commercial vendors. The approach can be followed by studies of any size, including those with datasets too large to process each record by hand and will promote consistency in" +
             "characterization of the retail food environment across studies.",
             "Data accuracy and completeness are crucial for ensuring both the correctness and epidemiologicalrelevance of a given" +
             "data set. In this study we evaluated a clinical register in the administrative district of Marburg-Biedenkopf, Germany, for these criteria. The register contained data gathered from a" +
             "comprehensive integrated breast-cancer network from three hospitals that treated all included incident cases of malignant breast cancer in two distinct time periods from 1996–97" +
             "and 2003–04 (N=488). To assess the accuracy of this data, we compared distributions of risk, prognostic, and predictive factors with distributions from established secondary databases to detect" +
             "any deviations from these “true” population parameters. To evaluate data completeness, we calculated epidemiological standardmeasures as well as incidence-mortality-ratios (IMRs). In total, 12%" +
             "(13 of 109) of the variables exhibited inaccuracies: . In contrast to raw, unstandardized incidence rates, directly age-standardized" +
             "incidence rates showed no systematic deviations. Our final completeness estimates were . Overall, the register contained accurate, complete, and correct data." +
             "Regional differences accounted for detected inaccuracies. Demographic shifts occurred. Age-standardized measures indicate an acceptable degree of completeness. The IMR method of measuring completeness" +
             "was inappropriate for incidence-based data registers. For the rising number of population-based health-care networks, further methodological advancements are necessary.Correct and epidemiologically relevant" +
             "data are crucial for clinical and health-policy decision-making.",
             "To determine the level of accuracy in coding for injury principal diagnosis and the first external" +
"cause code for public hospital discharges in New Zealand and determine how these levels vary by hospital size. A simple random sample of 1800 discharges was selected from the period 1996–98 inclusive." +
"Records were obtained from hospitals and an accredited coder coded the discharge independently of the codes already recorded in the national database. Five percent of the principal diagnoses, 18% of the first four digits" +
"of the E-codes, and 8% of the location codes (5th digit of the E-code), were incorrect. There were no substantive differences in the level of incorrect coding between large and small hospitals.Users of New Zealand public" +
"hospital discharge data can have a high degree of confidence in the injury diagnoses coded under ICD-9-CM-A. A similar degree of confidence is warranted for E-coding at the group level (for example, fall)," +
"but not, in general, at higher levels of specificity (for example, type of fall). For those countries continuing to use ICD-9 the study provides insight into potential problems of coding and thus guidance on where the" +
"focus of coder training should be placed. For those countries that have historical data coded according to ICD-9 it suggests that some specific injury and external cause incidence estimates may need to be treated with more caution.",
"Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable and can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging and must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size and its fast generation, it requires mechanisms and strategies to evaluate and assess data quality in a fast and efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness and consistency. The experimentations have been conducted on Sleep disorder’s data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data and illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality",
"While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the BigData value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.",
"This article investigates the evolution of data quality issues from traditional structured data managed in relational databases to Big Data. In particular, The paper examines the nature of the relationship between Data Quality and several research coordinates that are relevant in Big Data, such as the variety of data types, data sources and application domains, focusing on maps, semi-structured texts, linked open data, sensor & sensor networks and official statistics. Consequently a set of structural characteristics is identified and a systematization of the a posteriori correlation between them and quality dimensions is provided. Finally, Big Data quality issues are considered in a conceptual framework suitable to map the evolution of the quality paradigm according to three core coordinates that are significant in the context of the Big Data phenomenon: the data type considered, the source of data, and the application domain. Thus, the framework allows ascertaining the relevant changes in data quality emerging with the Big Data phenomenon, through an integrative and theoretical literature review.",
"To assess availability and completeness of data collected before and after a data quality audit(DQA) in voluntary medical male circumcision (VMMC) sites in Zimbabwe to determine the effect of this process on data quality. Data availability was measured as the percentage of VMMC clients whose CIF was on file at each site. A data evaluation tool measured the completeness of 34 key CIF variables. A comparison of pre-DQA and post-DQA results was conducted . After the DQA, high record availability of over 98 was maintained by sites 3 and 4. After the DQA, sites 1, 2 and 3 improved significantly in data completeness across 34 key indicators. For site 4, CIF data completeness decreased  after the DQA. Our findings suggest that CIF data availability and completeness generally improved after the DQA. However, gaps in documentation of vital signs and adverse events signal areas for improvement. Additional emphasis on data completeness would help support high-quality" +
"programme implementation and availability of reliable data for decision-making.",
"Globally, injury is a major cause of death and disability. Improvements in trauma care have been driven by trauma registries. The capacity of a trauma registry to inform improvements in the quality of trauma care is dependent upon the quality of data. The literature on data quality in disease registries is inconsistent and ambiguous; methods used for classifying, measuring, and improving data quality are not standardised. The aim of this study was to review the literature to determine the methods used to classify, measure and improve data quality in trauma registries. Methods: A scoping review of the literature was performed. Databases were searched using the term ‘‘trauma registry’’ and its synonyms, combined with multiple terms denoting data quality. There was no restriction on year. Full-length manuscripts were included if the classification, measurement or improvement of data quality in one or more trauma registries was a study objective. Data were abstracted regarding registry demographics, study design, data quality classification, and the reported methods used to measure and improve the pre-defined data quality dimensions of accuracy, completeness and capture. Results: Sixty-nine publications met the inclusion criteria. Four publications classified data quality. The most frequently described methods for measuring data accuracy were checks against other datasets and checks of injury coding. The most frequently described methods for measuring data completeness were the percentage of included cases, for a given variable or list of variables, for which there was an observation in the registry. The most frequently described methods for measuring data capture  were the percentage of cases in a linked reference dataset that were also captured in the primary dataset being evaluated . Most publications dealing with the measurement of a dimension of data quality did not specify the methods used; most publications dealing with the improvement of data quality did not specify the dimension being targeted. The classification, measurement and improvement of data quality in trauma registries is inconsistent. To maintain confidence in the usefulness of trauma registries, the metrics and reporting of data quality need to be standardised.",
"In Ireland, while detailed information is available regarding hospital attendance, little is known regarding general (family) practice attendance. However, it is conservatively estimated that there are almost nine times as many general practice encounters than there are hospital encounters each year in Ireland. This represents a very significant gap in health information. Indeed, general practice has been shown in other countries to be an important and rich source of information about the health of the population, their behaviors and their utilization of health services. Funded by the Health Information and Quality Authority (HIQA), the Irish College of General Practitioners (ICGP) undertook a feasibility study of diagnostic coding of routinely entered patient data and the creation of a national general practice morbidity and epidemiological database (GPMED project). This article outlines the process of data quality issue management undertaken. The study’s findings suggest that the quality of data collection and reporting structures available in general practice throughout Ireland at the outset of this project were not adequate to permit the creation of a database of sufficient quality for service planning and policy or epidemiological research. Challenges include the dearth of a minimum standard of data recorded in consultations by GPs and the absence of the digital data recording and exporting infrastructure within Irish patient management software systems. In addition, there is at present a lack of recognition regarding the value of such data for patient management and service planning—including importantly, data collectors who do not fully accept the merit of maintaining data, which has a direct consequence for data quality. The work of this project has substantial implications for the data available to the health sector in Ireland and contributes to the knowledge base internationally regarding general practice morbidity data.",
"Certain datasets that are incomplete or otherwise lacking in quality, such as maps and medical facility locations, may be enhanced using satellite image analysis by crowdsourcing or algorithms [Haklay and Weber 2008; Varshney et al. 2015]. Other datasets can be improved by similar means as for incidence data. The preceding datasets are of various type, provenance, spatial and temporal resolution and coverage, and quality. Many are accessible on the Internet but are not unified into a common repository. This activity uncovered approximately 50 heterogeneous data sources having the various quality challenges mentioned earlier. Redundancy and provenance are also issues, as some sources may reproduce or otherwise incorporate data from others. Some data sources we cataloged had terms that did not allow publishing; such a challenge may be addressed by contacting the owners and proposing alternative terms to which they may agree. Working with volunteers of different backgrounds and motivations resulted in varying levels of cataloging completeness and quality, which can be addressed in future events through more targeted volunteer selection and more initial training.Mobility of individuals. Mobile telephone penetration in Africa is very high, and call detail records (CDRs) provide an excellent source of information on population movements that can be used to predict disease spread and plan responses [Wesolowski et al. 2014]. CDRs can also aid in contact tracing—the identification of people who have recently interacted with infected individuals, which is a crucial task in controlling outbreaks. The primary mobile carrier in West Africa has a history of releasing CDRs in such a way that individual privacy is preserved but inferences drawn from the data release are valid [de Montjoye et al. 2014]. However, the governments of the affected countries have not yet permitted the release of (anonymized) CDRs. This challenge may be addressed through lobbying of the appropriate government ministries.",
"Data are important for making decisions. However, the quality of the data affects the quality of decisions. Data mining as one of the most important sources of knowledge needs high quality data to mine, but there are not enough good quality data in many enterprises. By analyzing the reasons for low data quality systematically, a new method called data mining consulting for improving data quality has been established. It defines data quality in a wider sense from the view of data mining, finds data quality problems, and solves data quality problems by a series of methods. Its application shows that it has good practicality and can increase data quality considerable.",
"Nowadays, activities and decisions making in an organization is based on data and information obtained from data analysis, which provides various services for constructing reliable and accurate process. As data are significant resources in all organizations the quality of data is critical for managers and operating processes to identify related performance issues. Moreover, high quality data can increase opportunity for achieving top services in an organization. However, identifying various aspects of data quality from definition, dimensions, types, strategies, techniques are essential to equip methods and processes for improving data. This paper focuses on systematic review of data quality dimensions in order to use at proposed framework which combining data mining and statistical techniques to measure dependencies among dimensions and illustrate how extracting knowledge can increase process quality.",
"Clinical Information Systems (CIS) have become a pivotal appliance in modern healthcare systems. Their adoption and increasing integration is driven by expectations related to better health outcomes and cost effectiveness. In practice, however, a lack of data quality (DQ) is often referred to as a significant inhibitor, impeding the full realization of these benefits. Although many authors have reported on DQ related problems, attaining and sustainingDQ in CIS has been a multi-faceted and elusive goal. The current literature on DQ in health informatics consists mainly of empirical studies and practitioners’ reports. Reports often focus on a particular issue or quality attribute but lack a holistic approach to addressing DQ ‘by-design’. This paper seeks to present a general framework for clinical DQ, which blends engineering theories with concepts and methods from health informatics. We define a new architectural viewpoint for designing and reasoning about DQ in CIS. We also introduce the notion of DQ Probes for monitoring and assuring DQ during system operation. Finally, we validate our framework with a real-world application case study.",
"Due to the complexity of genomic information and the broad amount of data produced every day, the genomic information accessible on the web has become very difficult to integrate, which hinders the research process. Using the knowledge from the Data Quality field and after a specific study of a set of genomic databases we have found problems related to six Data Quality dimensions. The aim of this paper is to highlight the problems that bioinformaticians have to face when they integrate information from different genomic databases. The contribution of this paper is to identify and characterize those problems in order to understand which ones hinder the research process, increasing the time-waste that this task means for researchers",
"In order to optimize access to the increasing amount of information, a classic solution has been data representation. The aim of this research is to uncover and systematize the factors and dimensions involved in the data representation issue and more exactly in the planning and design of the information products (IP) and their previous representation processes (RP). QFD (quality function deployment) is a planning tool based on user needs and expectations quality functions allowing the planning and design of IPs and RPs. A series of linked deployments provides the implied factors and dimensions: IP planning factors:representing, relating, filtering and seeking relevant information; IP design dimensions: relevance, format, comprehensiveness, consistency, accuracy and currentness; RP planning factors: comprehension, synthesizing, structuring and selecting; and RP design dimensions:human resources, computers and tangibles. By means of these deployments, the analysis of the factors and dimensions and their corresponding relationships provides an excellent picture for the quality planning and design of information products and representation processes",
"To review the methods and dimensions of data quality assessment in the context of electronic health record (EHR) data reuse for research. Materials and methods A review of the clinical research literature discussing data quality assessment methodology for EHR data was performed. Using an iterative process, the aspects of data quality being measured were abstracted and categorized, as well as the methods of assessment used. Five dimensions of data quality were identified, which are completeness, correctness, concordance, plausibility, and currency, and seven broad categories of data quality assessment methods: comparison with gold standards, data element agreement, data source agreement, distribution comparison, validity checks, logreview, and element presence.Examination of the methods by which clinical researchers have investigated the quality and suitability of EHR data for research shows that there are fundamental features of data quality, which may be difficult to measure, as well as proxy dimensions. Researchers interested in the reuse of EHR data for clinical research are recommended to consider the adoption of a consistent taxonomy of EHR data quality, to remain aware of the task dependence of data quality,to integrate work on data quality assessment from other fields, and to adopt systematic, empirically driven, statistically based methods of data quality assessment. There is currently little consistency or potential generalizability in the methods used to assess EHR data quality. If the reuse of EHR data for clinical research is to become accepted, researchers should adopt validated, systematic methods of EHR data quality assessment.",
"Assessing the quality of the information proposed by an information system has become one of the major research topics in the last two decades. A quick literature survey shows that a significant number of information quality frameworks are proposed in different domains of application: management information systems, web information systems, information fusion systems, and so forth. Unfortunately, they do not provide a feasible methodology that is both simple and intuitive to be implemented in practice. In order to address this need, we present in this article a new information quality methodology. Our methodology makes use of existing frameworks and proposes a three-step process capable of tracking the quality changes through the system. In the first step and as a novelty compared to existing studies, we propose decomposing the information system into its elementary modules. Having access to each module allows us to locally define the information quality. Then, in the second step, we model each processing module by a quality transfer function, capturing the module’s influence over the information quality. In the third step, we make use of the previous two steps in order to estimate the quality of the entire information system. Thus, our methodology allows informing the end-user on both output quality and local quality. The proof of concept of our methodology has been carried out considering two applications: an automatic target recognition system and a diagnosis coding support system.",
"Recording eye movement data with high quality is often a prerequisite for producing valid and replicable results and for drawing well-founded conclusions about the oculomotor system. Today, many aspects of data quality are often informally discussed among researchers but are very seldom measured, quantified, and reported. Here we systematically investigated how the calibration method, aspects of participants’ eye physiologies, the influences of recording time and gaze direction, and the experience of operators affect the quality of data recorded with a common tower-mounted, video-based eyetracker. We quantified accuracy, precision, and the amount of valid data, and found an increase in data quality when the participant indicated that he or she was looking at a calibration target, as compared to leaving this decision to the operator or the eyetracker software. Moreover, our results provide statistical evidence of how factors such as glasses, contact lenses, eye color, eyelashes, and mascara influence data quality. This method and the results provide eye movement researchers with an understanding of what is required to record high-quality data, as well as providing manufacturers with the knowledge to build better eyetrackers",
"Many health care systems and services exploit drug related information stored in databases. The poor data quality of these databases, e.g. inaccuracy of drug contraindications, can lead to catastrophic consequences for the health condition of patients. Hence it is important to ensure their quality in terms of data completeness and soundness. In the database domain, standard Functional Dependencies (FDs) and INclusion Dependencies (INDs), have been proposed to prevent the insertion of incorrect data. But they are generally not expressive enough to represent a domain-specific set of constraints. To this end, conditional dependencies, i.e. standard dependencies extended with tableau patterns containing constant values, have been introduced and several methods have been proposed for their discovery and representation. The quality of drug databases can be considerably improved by their usage. Moreover, pharmacology information is inherently hierarchical and many standards propose graph structures to represent them, e.g. the Anatomical Therapeutic Chemical classification (ATC) or Open-Galen’s terminology. In this article, we emphasize that the technologies of the Semantic Web are adapted to represent these hierarchical structures, i.e. in RDFS and OWL. We also present a solution for representing conditional dependencies using a query language defined for these graph oriented structures, namely SPARQL. The benefits of this approach are interoperability with applications and ontologies of the Semantic Web as well as a reasoning-based query execution solution to clean underlying databases",
"This study examines the relationships between the dimensions of information quality and healthcare quality. Past studies have confirmed a positive relationship between these two constructs, however, the relationships among the underlying dimensions of the constructs have not been explored. One of the primary purposes of this study is to show that different dimensions of information quality have different relationships with dimensions of healthcare quality. As the paper indicates, this development has implications for researchers and practitionersinterested in these two constructs",
"The query based on massive database is time-consuming and difficult. And the uneven quality of data source makes the multiple source selection more challenging. The low-quality data source can even make the result of the information unexpected. How to efficiently select quality-driven data sources on massive database remains a hard problem. In this paper, we study the efficient source selection problem on massive data set considering the quality of data sources. Our approach evaluates the quality of data source and balances the limitation of resources and the completeness of data source. For data source selection for a specific query, our method could select the data sources with the number of keywords larger than a given threshold. And the selected sources are ranked according to the values of information in data sources. Experimental results demonstrate that our method can scale to millions of data sources and perform pretty efficiently.",
"Health Authorities receive vast quantities of data from providers relating to patients treated. This data is used to survey the health of the resident population and to determine future healthcare services. It is therefore essential that the quality of this data is measured. North Staffordshire Health Authority already monitor, to a certain extent, the quality of data received. However, accuracy is one attribute of quality not monitored. This thesis proposes a method to measure the accuracy of patient data, in particular clinical coding. The traditional method of measuring accuracy determines whether a data item is correct or incorrect. The definition of accuracy, however, is the measure of agreement with an identified source. The proposed measure ranks incorrect clinical codes by their level of inaccuracy. Concepts from measurement theory are used to ensure that this measure adhered to the rules of the theory. This alternative method of measuring data accuracy was tested on a sample of inpatient data. From the results, the most appropriate way to analyse clinical data whilst still maintaining a level of accuracy satisfactory for the intended information purposes could be identified. Managers at North Staffordshire Health Authority were surveyed for their views on the usefulness of this alternative method of measuring data accuracy compared with the traditional method. Auditing a sample of data like this does not help to prevent errors occurring. Therefore, to identify how data accuracy could be improved in the long term, the source of the errors were discovered by examining the data life cycle.",
"The diffusion of Open Government Data (OGD) in recent years kept a very fast pace. However, evidence from practitioners shows that disclosing data without proper quality control may jeopardize dataset reuse and negatively affect civic participation. Current approaches to the problem in literature lack a comprehensive theoretical framework. Moreover, most of the evaluations concentrate on open data platforms, rather than on datasets. In this work, we address these two limitations and set up a framework of indicators to measure the quality of Open Government Data on a series of data quality dimensions at most granular level of measurement. We validated the evaluation framework by applying it to compare two cases of Italian OGD datasets: an internationally recognized good example of OGD, with centralized disclosure and extensive data quality controls, and samples of OGD from decentralized data disclosure (municipality level), with no possibility of extensive quality controls as in the former case, hence with supposed lower quality. Starting from measurements based on the quality framework, we were able to verify the difference in quality: the measures showed a few common acquired good practices and weaknesses, and a set of discriminating factors that pertain to the type of datasets and the overall approach. On the basis of this evaluation, we also provided technical and policy guidelines to overcome the weaknesses observed in the decentralized release policy, addressing specific quality aspects",
"Population health data, collected worldwide in an effort to monitor mortality and morbidity of mothers and babies, namely, perinatal data, are mandated at a federal level within Australia. The data are used to monitor patterns in midwifery, obstetric and neonatal practice, health outcomes, used for research purposes, funding allocation and education. Accuracy in perinatal data is most often reported via quantitative validation studies of perinatal data collections both internationally and in Australia. These studies report varying levels of accuracy and suggest researchers need to be more aware of the quality of data they use. This article presents findings regarding issues of concern identified by midwives relating to their perceptions of how technology affects the accuracy of perinatal data records. Perinatal data records are perceived to be more complete when completed electronically. However, issues regarding system functionality, the inconsistent use of terminology, lack of data standards and the absence of clear, written records contribute to midwives’ perceptions of the negative influence of technology on the quality of perinatal data",
"Information and its quality are impacted by numerous internal and external sources and dynamic factors, most of which influence them continually. Since information is a multidimensional construct, information quality improvement in an organization is required to consider various dimensions that are associated with degree of information quality. Therefore information quality dimensions are the fundamental components and indicators to assess and improve information quality in organizations. However information quality dimensions research is still immature to cover or identify all information aspects. This is because despite the vast information quality research by researchers and practitioners over the last two decades, little is known about the mutual relationships of information quality dimensions. In order to address the mutual relationships of information quality dimensions, the authors have three objectives: (1) identifying what types of attributes are embedded to information quality dimensions; (2) analysing how differently combinations of information quality dimensions impact quality of information; (3) discussing the relative importance of information quality dimensions from real-world practice. In this research, the authors conducted a case study employing analytic hierarchy process to fulfil the objectives",
"High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms",
"Data quality remains a persistent problem in practice and a challenge for research. In this study we focus on the four dimensions of data quality noted as the most important to information consumers, namely accuracy, completeness, consistency, and timeliness. These dimensions are of particular concern for operational systems, and most importantly for data warehouses, which are often used as the primary data source for analyses such as classification, a general type of data mining. However, the definitions and conceptual models of these dimensions have not been collectively considered with respect to data mining in general or classification in particular. Nor have they been considered for problem complexity. Conversely, these four dimensions of data quality have only been indirectly addressed by data mining research. Using definitions and constructs of data quality dimensions, our research evaluates the effects of both data quality and problem complexity on generated data and tests the results in a real-world case. Six different classification outcomes selected from the spectrum of classification algorithms show that data quality and problem complexity have significant main and interaction effects. From the findings of significant effects, the economics of higher data quality are evaluated for a frequent application of classification and illustrated by the real-world case",
"To describe the methodology for the development of data quality metrics in multi-institutional databases, deriving a cumulative data quality score [Aggregate Data Quality score (ADQ)]. The ESTS database was used to create and apply the metrics. The Units contributing to the ESTS database were ranked for the quality of data uploaded using the ADQ. We analysed data obtained from 96 Units contributing with at least 100 major lung resections ( January 2007 to December 2014). The Units were anonymized assigning a casual numeric code. The following metrics were developed for measuring the data quality of each Unit: (i) record Completeness (COM); rate of present variables on 16 expected variables for all the records uploaded (ii) record Reliability (REL); rate of consistent checks on 9 checks tested for all the records uploaded. These two metrics were rescaled using the mean and standard deviation of the entire dataset and summed, obtaining: (iii) ADQ score: [COM rescaled  REL rescaled]; it measures the cumulative data quality of a given dataset. The ADQ was used to rank the contributors. The COM of ESTS database contributors varied from 98.6 to 43% and the REL from 100 to 69%. Combining the rescaled metrics, the obtained ADQ ranged between 2.67 (highest data quality) and  (lowest data quality). Comparing the rating using just the COM value to the one obtained using the ADQ,  of Units changed their position. The major change was the drop of 66 positions considering the ADQ list. We described a reproducible method for data quality assessment in clinical multi-institutional databases. The ADQ is a unique indicator able to describe data quality and to compare it among centres. It has the potential of objectively guiding projects of data quality management and improvement.",
"Poor data quality can be a serious threat to the validity and generalizability of clinical research findings. The growing availability of electronic administrative and clinical data is accompanied by a growing concern about the quality of these data for observational research and other analytic purposes. Currently, there are no widely accepted guidelines for reporting quality results that would enable investigators and consumers to independently determine if a data source is fit for use to support analytic inferences and reliable evidence generation. We developed a conceptual model that captures the flow of data from data originator across successive data stewards and finally to the data consumer. This “data lifecycle” model illustrates how data quality issues can result in data being returned back to previous data custodians. We highlight the potential risks of poor data quality on clinical practice and research results. Because of the need to ensure transparent reporting of a data quality issues, we created a unifying data-quality reporting framework and a complementary set of 20 data-quality reporting recommendations for studies that use observational clinical and administrative data for secondary data analysis. We obtained stakeholder input on the perceived value of each recommendation by soliciting public comments via two face-to-face meetings of informatics and comparative-effectiveness investigators, through multiple public webinars targeted to the health services research community, and with an open access online wiki. Our recommendations propose reporting on both general and analysis-specific data quality features. The goals of these recommendations are to improve the reporting of data quality measures for studies that use observational clinical and administrative data, to ensure transparency and consistency in computing data quality measures, and to facilitate best practices and trust in the new clinical discoveries based on secondary use of observational data.",
"This project seeks to reduce duplication of effort in finding data for NHS healthcare quality indicators, to resolve issues identified in previous efforts to develop quality-monitoring ontologies and to identify areas for future computer-interpretable quality indicator development for the United Kingdom’s Department of Health and National Health Service (NHS). Outcomes will include specification of inclusion and exclusion criteria for a set of healthcare quality indicators, along with categorisation beyond screening and prevention and identification of levels of indicator relationships. Following an exploration of potential methods for ontology development, Methontology was the method chosen to develop the ontology. This involved a conceptual analysis to inform the development of an ontology for a 2009 set of healthcare quality indicators made available on the NHS Information Centre website. Indicators were categorised by NHS Dimension, NHS-specified clinical pathway and by United States Institute of Medicine purpose. Relationships between indicators were identified, as well as an initial set of inclusion and exclusion criteria. NHS quality indicators that share some of the same criteria were made searchable, along with broader and narrower related criteria. Up to six layers of inclusion and exclusion criteria were specified and incorporated into the ontology. Search capabilities were created for indicators originating from the same source and from more than one source, along with indicators assigned to specific care pathways. It was shown that indicators have purposes other than prevention and screening, rendering Arden Syntax, intended for computer-interpretable guidelines and previously tested on a specialised set of healthcare quality indicators, unsuitable for a large, diverse set of quality indicators. A large number, 222, of quality indicators with different purposes justified the development of a separate ontology. This ontology could reduce duplication of effort in finding data for NHS healthcare quality indicators. There is potential to link to components of queries currently in use in the NHS, as an interim step away from the need to develop separate queries for each indicator. Areas for future computer interpretable quality indicator development include resolving Electronic Health" +
"Record compatibility issues and improved indicator metadata quality. The ontology could be useful to NHS indicator developers, NHS data extractors and vendors of electronic health records who supply to the NHS",
"The rapid accumulation of genome annotations, as well as their widespread reuse in clinical and scientific practice, poses new challenges to management of the quality of scientific data. This study contributes towards better understanding of scientist perception and priorities for data quality and data quality assurance skills needed in genome annotation. Our study was guided by a previously developed general framework for assessment of data quality and by a taxonomy of data quality skills, and intended to define context sensitive models of criteria for data quality and skills for genome annotation. Analysis of the results revealed that genomics scientists recognize specific sets of criteria for quality in the genome annotation context. Seventeen data quality dimensions were reduced to five factor constructs, and 17 relevant skills were grouped into four factor constructs. The constructs defined by this study advances the understanding of data quality relationships and is an important contribution to data and information quality research. In addition, the resulting models can serve as valuable resources to genome data curators and administrators for developing data curation policies and designing DQ assurance strategies, processes, procedures, and infrastructure.",
"Improving data quality is a basic step for all companies and organizations as it leads to increase opportunity to achieve top services. The aim of this study was to validate and adapt the four major data quality dimensions’ instruments in different information systems. The four important quality dimensions which were used in this study were; Accuracy, Completeness, Consistency and Timeliness. The questionnaire was developed, validated and used for collecting data on the different information system’s users. A set of questionnaire was conducted with 50 respondents who using different information systems. Inferential statistics and descriptive analysis were employed to measure and validate the factor contributing to the quality improvement process. This study has been compared with related parts of previous studies; and showed that the instrument is valid to measure quality dimensions and improvement process. The content validity, reliability and factor analysis were applied on 24 items to compute the results. The results showed that the instrument is considered to be reliable and validated. The results also suggest that the instrument can be used as a basic foundation to implicate data quality for an organization's manager to design improvement process. ",
"The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed.We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario.We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases",
"A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study’s findings, the “truth about Big Data” is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit returns-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.",
"Big Data (BD) is everywhere and quite a lot of benefits have been derived from its usage by different organizations. Notwithstanding, there are still numerous tech-nical and research challenges that must be tackled to comprehend and gain its full poten-tial. The major challenges of BD are not just its processing, storage and analytics, there are also challenges associated with that run across the BD value chain such as the data collection phase, integration and the enforcement of quality. This paper propose a DQ transformation model to evaluate BD quality from the data collection phase through to the visualization phase involving both data-driven and process-driven quality evaluation by assessing the quality of data itself first then assessing the process quality. This is still an ongoing research and hopefully will be experimented using specific Data Quality Di-mensions (DQDs) like completeness, consistency, accuracy and timeliness with process quality dimensions such as Throughput, response time, latency with their corresponding metrics. Some classification algorithms too will be implemented in the processing and analytics phase of the BD chain and finally quality of representation and user satisfaction will be measured at the visualization phase.",
"With the rapid development of social networks and Internet of things, Big Data age has arrived. The increasing number of data have brought great value to the public and enterprises, how to manage and use Big Data better has become the focus of all walks of life. However, the Big Data 4V characteristics has brought a lot of problems to the processing of Big Data. The key to Big Data processing is to solve the problem of data quality, to ensure data quality as a prerequisite for Big Data to play the value. The recommendation system and the prediction system are the successful application of the Big Data technology. In this paper, we study the recommendation system and prediction system in Big Data environment, and try to find out the data quality of data collection, data preprocessing, data storage and data analysis in Big Data processing. Through the elaboration and analysis of the problem, the corresponding solution is put forward. At the end of the paper we have raised some open questions",
"Big data applications are currently used in many application domains, ranging from statistical applications to prediction systems and smart cities. However, the quality of these applications is far from perfect, such as functional error, failure and low performance. Consequently, assuring the overall quality for big data applications plays an increasingly important role. This paper aims at summarizing and assessing existing quality assurance (QA) technologies addressing quality issues in big data applications. We have conducted a systematic literature review (SLR) by searching major scientific databases, resulting in 83 primary and relevant studies on QA technologies for big data applications. The SLR results reveal the following main findings: (1) the quality attributes that are focused for the quality of big data applications, including correctness, performance, availability, scalability and reliability, and the factors influencing them; (2) the existing implementation-specific QA technologies, including specification, architectural choice and fault tolerance, and the process-specific QA technologies, including analysis, verification, testing, monitoring and fault and failure prediction; (3) existing strengths and limitations of each kind of QA technology; (4) the existing empirical evidence of each QA technology. This study provides a solid foundation for research on QA technologies of big data applications and can help developers of big data applications apply suitable QA technologies",
"Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity",
"Ensuring the quality of data in healthcare is important as to determine the level of performance of health services offered by them. The key factor that helps boost their performance are the enhancement of timely diagnosis and management of diseases and patient, keeping personnel satisfaction and lessen the hospital costs in running them. Assessing the quality of data in adoption electronic health record (EHR) and preventing the rising of patient readmission rates in Malaysia is the main concern of the project. However, there is lack of study on patient readmission factors in Malaysia and the adoption of electronic health record (EHR) also causing poor quality of patients’ data to be analyzed. Lastly, delivering the information to patients is difficult and confusing because of lacking in medical knowledge and terms. Therefore, this paper proposes consistency as additional dimension for data quality dimension to be used for validating patient readmission dataset. Theoretically, this project is seen to improve the quality of information by proposing improvement in used dimension to assess data quality of EHR. On the other hand, practically, this paper will help to visualize the results of medicinal information in the form of dashboard, which can be easily understood by both patients and health practitioners",
"Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities",
"The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics.We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research"]
# remove common words and tokenize
stoplist = set('for a of the and to namely higher in on at data their ours Ours yours her his and from other are with such but require is care We we They these using over can that towards within between known be users 0 1 2 3 4 5 6 7 8 9 & first were was also %'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once

from collections import defaultdict
frequency = defaultdict(int)
for text in texts:
     for token in text:
        frequency[token] += 1

texts = [[token for token in text if frequency[token] > 1]
       for text in texts]

#from pprint import pprint  # pretty-printer
#pprint(texts)

dictionary = corpora.Dictionary(texts)
#dictionary.save('testdictionary.dict')  # store the dictionary, for future reference
#print(dictionary)



#print(new_vec)  # the word "interaction" does not appear in the dictionary and is ignored

corpus = [dictionary.doc2bow(text) for text in texts]


tfidf = models.TfidfModel(corpus)
#doc_bow = [(0, 1), (1, 1)]
#print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors

corpus_tfidf = tfidf[corpus]
#for doc in corpus_tfidf:
#    print(doc)

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10) # initialize an LSI transformation
#corpus_lsi = lsi[corpus_tfidf]
#lsi.print_topics(2)

#lsi.save('model.lsi') # same for tfidf, lda, ...
#lsi = models.LsiModel.load('model.lsi')

doc = "Duplication" #terms we are evaluating via LSA
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow] # convert the query to LSI space
#print(vec_lsi)
index = similarities.MatrixSimilarity(lsi[corpus])

#persisting the index


sims = index[vec_lsi] # perform a similarity query against the corpus
sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims) # print sorted (document number, similarity score) 2-tuples
